"""Streamlit app for browsing banking regulations and offers using Gemini."""
from __future__ import annotations

import os
from textwrap import dedent
from typing import Optional

import pandas as pd
import streamlit as st

try:
    import google.generativeai as genai
except ImportError as exc:  # pragma: no cover - streamlit runtime dependency
    raise ImportError(
        "The google-generativeai package is required. Install dependencies with `pip install -r requirements.txt`."
    ) from exc


DATA = pd.DataFrame(
    {
        "id": range(1, 6),
        "type": ["R√®glementation", "R√®glementation", "Offre", "Offre", "Contrat"],
        "titre": [
            "Dur√©e maximale d'un cr√©dit immobilier",
            "Conditions d‚Äôoctroi d‚Äôun cr√©dit consommation",
            "Livret A",
            "Assurance Vie",
            "Conditions g√©n√©rales Compte Courant",
        ],
        "description": [
            "La dur√©e maximale d‚Äôun cr√©dit immobilier est de 25 ans.",
            "Un cr√©dit √† la consommation ne peut pas exc√©der 75 000‚Ç¨.",
            "Taux r√©glement√©, plafond fix√© par l'√âtat.",
            "Produit d‚Äô√©pargne √† long terme, fiscalit√© avantageuse.",
            "R√®gles g√©n√©rales applicables √† la gestion des comptes bancaires.",
        ],
    }
)


@st.cache_data(show_spinner=False)
def get_dataset() -> pd.DataFrame:
    """Return the static dataset as a DataFrame."""
    return DATA.copy()


def build_context(df: pd.DataFrame) -> str:
    """Return a formatted context string for Gemini."""
    lines = [
        f"{row.type} : {row.titre}. {row.description}"
        for row in df.itertuples()
    ]
    return "\n".join(lines)


def get_api_key_from_config() -> Optional[str]:
    """Fetch the Gemini API key from Streamlit secrets or environment variables."""
    return st.secrets.get("GEMINI_API_KEY", os.getenv("GEMINI_API_KEY"))


def resolve_api_key() -> Optional[str]:
    """Return the stored manual key or fall back to configuration."""
    stored_key: Optional[str] = st.session_state.get("stored_api_key")
    return stored_key or get_api_key_from_config()


@st.cache_resource(show_spinner=False)
def load_model(api_key: str):
    """Configure and return the Gemini model instance."""
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash")


def ask_gemini(model, question: str, context: str) -> str:
    """Generate a response from Gemini using the curated context."""
    prompt = dedent(
        f"""
        Tu es un assistant sp√©cialis√© en r√©glementation bancaire et offres produits.
        Voici les donn√©es disponibles :
        {context}

        Question : {question}

        R√©ponds clairement en citant uniquement les √©l√©ments pertinents (titre + r√©sum√©).
        Ajoute aussi un nombre total de r√©sultats si applicable.
        """
    ).strip()

    response = model.generate_content(prompt)
    return response.text


def main() -> None:
    st.set_page_config(
        page_title="Assistant R√©glementation & Offres",
        page_icon="üìë",
        layout="wide",
    )

    st.title("üìë Assistant R√©glementation & Offres")
    st.write(
        "Posez vos questions sur les r√®gles, contrats ou offres bancaires comme "
        "dans un salon de discussion. Les r√©ponses s'appuient sur les fiches internes disponibles."
    )

    df = get_dataset()
    context = build_context(df)

    with st.sidebar:
        st.header("üîç Filtrer le r√©f√©rentiel")
        type_selection = st.multiselect(
            "Types √† afficher",
            options=sorted(df["type"].unique()),
            default=list(sorted(df["type"].unique())),
        )

        st.markdown("---")
        st.header("üîê Cl√© API Gemini")
        st.caption(
            "Collez votre cl√© API personnelle pour interroger Gemini."
        )

        stored_api_key = st.session_state.get("stored_api_key", "")
        manual_key = st.text_input(
            "Cl√© API",
            value=stored_api_key,
            type="password",
            help="La cl√© reste stock√©e uniquement dans votre session Streamlit.",
        )

        normalized_key = manual_key.strip() if manual_key else ""
        if normalized_key and normalized_key != stored_api_key:
            st.session_state["stored_api_key"] = normalized_key
            st.session_state.pop("model", None)
            st.session_state.pop("messages", None)
            st.success("Cl√© API pr√™te √† l'emploi.")
        elif not normalized_key and stored_api_key:
            st.session_state.pop("stored_api_key", None)
            st.session_state.pop("model", None)
            st.session_state.pop("messages", None)

        st.caption(
            "Vous pouvez aussi d√©finir la cl√© via `.streamlit/secrets.toml` ou la variable "
            "d'environnement `GEMINI_API_KEY`."
        )

    filtered_df = df[df["type"].isin(type_selection)] if type_selection else df
    context = build_context(filtered_df)

    st.subheader("üìö R√©f√©rentiel disponible")
    st.dataframe(
        filtered_df,
        use_container_width=True,
        hide_index=True,
    )

    if filtered_df.empty:
        st.warning(
            "Aucun document ne correspond aux filtres s√©lectionn√©s. "
            "Ajustez-les pour pouvoir interroger l'assistant."
        )
        return

    st.markdown("---")

    api_key = resolve_api_key()

    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    st.markdown("### üí¨ Conversation")
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if not api_key:
        st.info(
            "‚ö†Ô∏è Ajoutez votre cl√© API Gemini pour discuter avec l'assistant."
        )
        return

    if "model" not in st.session_state:
        try:
            st.session_state["model"] = load_model(api_key)
        except Exception as error:  # pragma: no cover - handled at runtime
            st.error(f"Impossible d'initialiser le mod√®le Gemini : {error}")
            return

    prompt = st.chat_input("Posez votre question sur les produits bancaires‚Ä¶")

    if prompt:
        st.session_state["messages"].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Consultation du mod√®le Gemini..."):
                try:
                    answer = ask_gemini(st.session_state["model"], prompt.strip(), context)
                except Exception as error:  # pragma: no cover - handled at runtime
                    st.error(f"Une erreur est survenue lors de l'appel √† Gemini : {error}")
                else:
                    st.markdown(answer)
                    st.session_state["messages"].append(
                        {"role": "assistant", "content": answer}
                    )


if __name__ == "__main__":
    main()
